{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4c5a8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import hstack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6d1582d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load Data\n",
    "df_train = pd.read_csv('../resource/Mental-Health-Twitter-Tokenized/train.csv')\n",
    "df_val = pd.read_csv('../resource/Mental-Health-Twitter-Tokenized/val.csv')\n",
    "df_test = pd.read_csv('../resource/Mental-Health-Twitter-Tokenized/test.csv')\n",
    "\n",
    "# Prepare the features and target variable\n",
    "X_train_text = df_train['processed_tokens']\n",
    "X_val_text = df_val['processed_tokens']\n",
    "X_test_text = df_test['processed_tokens']\n",
    "\n",
    "y_train = df_train['label']\n",
    "y_val = df_val['label']\n",
    "y_test = df_test['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78ad4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nanphattongsirisukool/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV] END clf__max_depth=10, clf__max_features=log2, clf__min_samples_split=5, clf__n_estimators=100; total time=   1.2s\n",
      "[CV] END clf__max_depth=10, clf__max_features=log2, clf__min_samples_split=5, clf__n_estimators=100; total time=   1.1s\n",
      "[CV] END clf__max_depth=10, clf__max_features=log2, clf__min_samples_split=5, clf__n_estimators=100; total time=   1.2s\n",
      "[CV] END clf__max_depth=10, clf__max_features=log2, clf__min_samples_split=2, clf__n_estimators=500; total time=   3.4s\n",
      "[CV] END clf__max_depth=10, clf__max_features=log2, clf__min_samples_split=2, clf__n_estimators=500; total time=   3.5s\n",
      "[CV] END clf__max_depth=10, clf__max_features=log2, clf__min_samples_split=2, clf__n_estimators=500; total time=   3.6s\n",
      "[CV] END clf__max_depth=10, clf__max_features=sqrt, clf__min_samples_split=2, clf__n_estimators=300; total time=   2.4s\n",
      "[CV] END clf__max_depth=10, clf__max_features=sqrt, clf__min_samples_split=2, clf__n_estimators=300; total time=   2.6s\n",
      "[CV] END clf__max_depth=10, clf__max_features=sqrt, clf__min_samples_split=10, clf__n_estimators=500; total time=   4.2s\n",
      "[CV] END clf__max_depth=10, clf__max_features=sqrt, clf__min_samples_split=10, clf__n_estimators=500; total time=   4.2s\n",
      "[CV] END clf__max_depth=10, clf__max_features=sqrt, clf__min_samples_split=10, clf__n_estimators=500; total time=   3.8s\n",
      "[CV] END clf__max_depth=10, clf__max_features=sqrt, clf__min_samples_split=2, clf__n_estimators=300; total time=   2.3s\n",
      "[CV] END clf__max_depth=10, clf__max_features=log2, clf__min_samples_split=10, clf__n_estimators=300; total time=   1.7s\n",
      "[CV] END clf__max_depth=10, clf__max_features=sqrt, clf__min_samples_split=5, clf__n_estimators=500; total time=   3.5s\n",
      "[CV] END clf__max_depth=10, clf__max_features=sqrt, clf__min_samples_split=5, clf__n_estimators=500; total time=   3.7s\n",
      "[CV] END clf__max_depth=10, clf__max_features=sqrt, clf__min_samples_split=5, clf__n_estimators=500; total time=   3.6s\n",
      "[CV] END clf__max_depth=10, clf__max_features=log2, clf__min_samples_split=10, clf__n_estimators=300; total time=   2.0s\n",
      "[CV] END clf__max_depth=10, clf__max_features=log2, clf__min_samples_split=10, clf__n_estimators=300; total time=   1.9s\n",
      "[CV] END clf__max_depth=30, clf__max_features=log2, clf__min_samples_split=10, clf__n_estimators=100; total time=   1.4s\n",
      "[CV] END clf__max_depth=30, clf__max_features=log2, clf__min_samples_split=10, clf__n_estimators=100; total time=   1.2s\n",
      "[CV] END clf__max_depth=30, clf__max_features=log2, clf__min_samples_split=10, clf__n_estimators=100; total time=   1.2s\n",
      "[CV] END clf__max_depth=30, clf__max_features=sqrt, clf__min_samples_split=2, clf__n_estimators=500; total time=   8.8s\n",
      "[CV] END clf__max_depth=30, clf__max_features=sqrt, clf__min_samples_split=2, clf__n_estimators=500; total time=   9.1s\n",
      "[CV] END clf__max_depth=30, clf__max_features=sqrt, clf__min_samples_split=2, clf__n_estimators=500; total time=   9.1s\n",
      "[CV] END clf__max_depth=30, clf__max_features=sqrt, clf__min_samples_split=10, clf__n_estimators=500; total time=   7.6s\n",
      "[CV] END clf__max_depth=30, clf__max_features=sqrt, clf__min_samples_split=10, clf__n_estimators=500; total time=   7.7s\n",
      "[CV] END clf__max_depth=30, clf__max_features=sqrt, clf__min_samples_split=10, clf__n_estimators=500; total time=   7.7s\n",
      "[CV] END clf__max_depth=None, clf__max_features=sqrt, clf__min_samples_split=2, clf__n_estimators=500; total time= 1.1min\n",
      "[CV] END clf__max_depth=None, clf__max_features=sqrt, clf__min_samples_split=2, clf__n_estimators=500; total time= 1.1min\n",
      "[CV] END clf__max_depth=None, clf__max_features=sqrt, clf__min_samples_split=2, clf__n_estimators=500; total time= 1.0min\n",
      "Final Test Set Evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.70      0.72      1482\n",
      "           1       0.72      0.75      0.73      1483\n",
      "\n",
      "    accuracy                           0.72      2965\n",
      "   macro avg       0.72      0.72      0.72      2965\n",
      "weighted avg       0.72      0.72      0.72      2965\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "\n",
    "# Download stopwords if not already\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Step 1: Load Data\n",
    "df_train = pd.read_csv('../resource/Mental-Health-Twitter-Tokenized/train.csv')\n",
    "df_val = pd.read_csv('../resource/Mental-Health-Twitter-Tokenized/val.csv')\n",
    "df_test = pd.read_csv('../resource/Mental-Health-Twitter-Tokenized/test.csv')\n",
    "\n",
    "# Combine train and val for RandomizedSearchCV tuning\n",
    "df_full = pd.concat([df_train, df_val], ignore_index=True)\n",
    "\n",
    "X_full_text = df_full['processed_tokens']\n",
    "y_full = df_full['label']\n",
    "X_test_text = df_test['processed_tokens']\n",
    "y_test = df_test['label']\n",
    "\n",
    "# Step 2: Improved Text Preprocessing\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def clean_tokens(token_str):\n",
    "    tokens = eval(token_str)\n",
    "    cleaned = [\n",
    "        stemmer.stem(w.lower())\n",
    "        for w in tokens\n",
    "        if w.isalpha() and w.lower() not in stop_words\n",
    "    ]\n",
    "    return ' '.join(cleaned)\n",
    "\n",
    "X_full_text = X_full_text.apply(clean_tokens)\n",
    "X_test_text = X_test_text.apply(clean_tokens)\n",
    "\n",
    "# Step 3: Build a Pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=5000)),\n",
    "    ('clf', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Step 4: Hyperparameter Tuning\n",
    "param_dist = {\n",
    "    'clf__n_estimators': [100, 300, 500],\n",
    "    'clf__max_depth': [10, 30, None],\n",
    "    'clf__min_samples_split': [2, 5, 10],\n",
    "    'clf__max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    pipeline,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10,\n",
    "    cv=3,\n",
    "    scoring='f1_weighted',\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Step 5: Train with Hyperparameter Tuning\n",
    "search.fit(X_full_text, y_full)    \n",
    "\n",
    "# Step 6: Evaluate on Test Set\n",
    "y_test_pred = search.predict(X_test_text)\n",
    "print(\"Final Test Set Evaluation:\")\n",
    "print(classification_report(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d883ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_combined = X_train_tfidf\n",
    "X_val_combined = X_val_tfidf\n",
    "X_test_combined = X_test_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6e8b4622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Numeric Features (followers, retweets, etc.)\n",
    "numeric_features_train = df_train[['followers', 'friends', 'favourites', 'statuses', 'retweets', 'URLs', 'Mentions']]\n",
    "numeric_features_val = df_val[['followers', 'friends', 'favourites', 'statuses', 'retweets', 'URLs', 'Mentions']]\n",
    "numeric_features_test = df_test[['followers', 'friends', 'favourites', 'statuses', 'retweets', 'URLs', 'Mentions']]\n",
    "\n",
    "# Standardize the numeric features\n",
    "scaler = StandardScaler()\n",
    "X_train_numeric = scaler.fit_transform(numeric_features_train)\n",
    "X_val_numeric = scaler.transform(numeric_features_val)\n",
    "X_test_numeric = scaler.transform(numeric_features_test)\n",
    "\n",
    "# Step 4: Combine Text and Numeric Features\n",
    "X_train_combined = X_train_tfidf\n",
    "X_val_combined = X_val_tfidf\n",
    "X_test_combined = X_test_tfidf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "56083b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set Evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.72      0.72      1481\n",
      "           1       0.72      0.73      0.73      1484\n",
      "\n",
      "    accuracy                           0.72      2965\n",
      "   macro avg       0.72      0.72      0.72      2965\n",
      "weighted avg       0.72      0.72      0.72      2965\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Train the Model\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train_combined, y_train)\n",
    "\n",
    "# Step 6: Validation - Evaluate on the validation set\n",
    "y_val_pred = model.predict(X_val_combined)\n",
    "print(\"Validation Set Evaluation:\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "# # Step 7: After Validation, Train on the Full Training + Validation Data\n",
    "# X_full_train = hstack([X_train_combined, X_val_combined])\n",
    "# y_full_train = pd.concat([y_train, y_val], axis=0)\n",
    "\n",
    "# # Re-train the model on the full training + validation data\n",
    "# model.fit(X_full_train, y_full_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f9e5a851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Evaluation (Final Model):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.71      0.71      1482\n",
      "           1       0.71      0.72      0.71      1483\n",
      "\n",
      "    accuracy                           0.71      2965\n",
      "   macro avg       0.71      0.71      0.71      2965\n",
      "weighted avg       0.71      0.71      0.71      2965\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Final Testing - Evaluate on the test set\n",
    "y_test_pred = model.predict(X_test_combined)\n",
    "print(\"Test Set Evaluation (Final Model):\")\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bfb563",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "studysession",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
