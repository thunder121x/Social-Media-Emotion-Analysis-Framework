{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60a918d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/nanphattongsirisukool/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "import contractions\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "\n",
    "nltk.download('words')\n",
    "standard_words = set(words.words())\n",
    "\n",
    "# Load slang dictionary\n",
    "with open('resource/slang.json', 'r', encoding='utf-8') as f:\n",
    "    slang_dict = json.load(f)\n",
    "\n",
    "def preprocess_twitter_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    drop_cols = ['Unnamed: 0', 'post_id', 'post_created', 'user_id', \n",
    "                 'followers', 'friends', 'favourites', 'statuses', 'retweets']\n",
    "    df = df.drop(columns=[col for col in drop_cols if col in df.columns], errors='ignore')\n",
    "\n",
    "    # Drop duplicates\n",
    "    df = df.drop_duplicates(keep=False)\n",
    "\n",
    "    # Step 1: Remove URLs and mark presence\n",
    "    def remove_urls(text):\n",
    "        pattern = r'http\\S+|www\\S+'\n",
    "        return re.sub(pattern, '', text), int(bool(re.search(pattern, text)))\n",
    "    df[['post_text', 'URLs']] = df['post_text'].apply(lambda x: pd.Series(remove_urls(x)))\n",
    "\n",
    "    # Step 2: Remove Mentions and mark presence\n",
    "    def remove_mentions(text):\n",
    "        pattern = r'@\\w+'\n",
    "        return re.sub(pattern, '', text), int(bool(re.search(pattern, text)))\n",
    "    df[['post_text', 'Mentions']] = df['post_text'].apply(lambda x: pd.Series(remove_mentions(x)))\n",
    "\n",
    "    # Step 3: Extract Hashtags and remove from text\n",
    "    def extract_hashtags(text):\n",
    "        return re.findall(r'#\\w+', text)\n",
    "    df['Hashtags'] = df['post_text'].apply(extract_hashtags)\n",
    "    df['post_text'] = df['post_text'].apply(lambda x: re.sub(r'#\\w+', '', x))\n",
    "\n",
    "    # Step 4: Convert emojis\n",
    "    def convert_emojis(text):\n",
    "        text = emoji.demojize(text)\n",
    "        text = re.sub(r':([a-zA-Z_]+):', r' \\1 ', text)\n",
    "        return re.sub(r'\\s+', ' ', text.replace('_', ' ')).strip()\n",
    "    df['post_text'] = df['post_text'].apply(convert_emojis)\n",
    "\n",
    "    # Step 5: Expand contractions\n",
    "    df['post_text'] = df['post_text'].apply(contractions.fix)\n",
    "\n",
    "    # Step 6: Remove special characters and lowercase\n",
    "    df['post_text'] = df['post_text'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x.lower()))\n",
    "\n",
    "    # Step 7: Normalize slang\n",
    "    def normalize_slang(text):\n",
    "        return ' '.join([slang_dict.get(w, w) if w.lower() not in standard_words else w \n",
    "                         for w in text.split()])\n",
    "    df['post_text'] = df['post_text'].apply(normalize_slang)\n",
    "\n",
    "    # Step 8: One-hot encode hashtags\n",
    "    all_hashtags = list(set(h for tags in df['Hashtags'] for h in tags))\n",
    "    for tag in all_hashtags:\n",
    "        clean_tag = tag[1:]  # remove '#'\n",
    "        df[f'hashtag_{clean_tag}'] = df['Hashtags'].apply(lambda tags: int(tag in tags))\n",
    "\n",
    "    # Drop Hashtags column\n",
    "    df = df.drop(columns=['Hashtags'])\n",
    "\n",
    "    # Ensure label is last column if it exists\n",
    "    if 'label' in df.columns:\n",
    "        label_col = df.pop('label')\n",
    "        df['label'] = label_col\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f58ec557",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "def undersample(df, label_col='label', random_state=42):\n",
    "    # Separate majority and minority classes\n",
    "    df_majority = df[df[label_col] == 0]\n",
    "    df_minority = df[df[label_col] == 1]\n",
    "\n",
    "    # Downsample majority class to match minority class\n",
    "    df_majority_downsampled = resample(\n",
    "        df_majority,\n",
    "        replace=False,  # without replacement\n",
    "        n_samples=len(df_minority),\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Combine minority class with downsampled majority class\n",
    "    df_balanced = pd.concat([df_majority_downsampled, df_minority])\n",
    "\n",
    "    # Shuffle the result\n",
    "    df_balanced = df_balanced.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "\n",
    "    return df_balanced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d91015e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>im getting into borderlands and i can murder y...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74676</th>\n",
       "      <td>Just realized that the Windows partition of my...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74677</th>\n",
       "      <td>Just realized that my Mac window partition is ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74678</th>\n",
       "      <td>Just realized the windows partition of my Mac ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74679</th>\n",
       "      <td>Just realized between the windows partition of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74680</th>\n",
       "      <td>Just like the windows partition of my Mac is l...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74681 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               post_text  label\n",
       "0      I am coming to the borders and I will kill you...      0\n",
       "1      im getting on borderlands and i will kill you ...      0\n",
       "2      im coming on borderlands and i will murder you...      0\n",
       "3      im getting on borderlands 2 and i will murder ...      0\n",
       "4      im getting into borderlands and i can murder y...      0\n",
       "...                                                  ...    ...\n",
       "74676  Just realized that the Windows partition of my...      0\n",
       "74677  Just realized that my Mac window partition is ...      0\n",
       "74678  Just realized the windows partition of my Mac ...      0\n",
       "74679  Just realized between the windows partition of...      0\n",
       "74680  Just like the windows partition of my Mac is l...      0\n",
       "\n",
       "[74681 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file and select columns 2 and 3 (0-based index)\n",
    "df = pd.read_csv('twitter_training.csv').iloc[:,2:4]\n",
    "\n",
    "# Set the column names to 'label' and 'post_text'\n",
    "df.columns = ['label', 'post_text']\n",
    "\n",
    "# Drop the 'label' column\n",
    "df['label'] = df.pop('label')\n",
    "\n",
    "target = 'Negative'\n",
    "df['label'] = (df['label'] == target).astype(int)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "# df[df['label'] == 1]\n",
    "\n",
    "# df = undersample(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78a3b05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = preprocess_twitter_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f31ca9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/nanphattongsirisukool/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/nanphattongsirisukool/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/nanphattongsirisukool/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nanphattongsirisukool/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Ensure required resources are available\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def tokenize_and_lemmatize(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    # Step 1: Tokenize\n",
    "    df['tokens'] = df['post_text'].apply(lambda x: word_tokenize(str(x).lower()))\n",
    "\n",
    "    # Step 2: Remove stopwords\n",
    "    df['processed_tokens'] = df['tokens'].apply(lambda tokens: [word for word in tokens if word not in stop_words])\n",
    "\n",
    "    # Step 3: Lemmatize with POS tagging\n",
    "    def get_wordnet_pos(tag):\n",
    "        if tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN\n",
    "\n",
    "    def lemmatize_tokens(tokens):\n",
    "        tagged = pos_tag(tokens)\n",
    "        return [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in tagged]\n",
    "\n",
    "    df['processed_tokens'] = df['processed_tokens'].apply(lemmatize_tokens)\n",
    "    \n",
    "    # Drop columns 'post_text' and 'tokens'\n",
    "    df = df.drop(['post_text', 'tokens'], axis=1)\n",
    "    # print(df)\n",
    "\n",
    "    # Reorder label to last column if exists\n",
    "    if 'label' in df.columns:\n",
    "        label = df.pop('label')\n",
    "        df['label'] = label\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "424cfd73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URLs</th>\n",
       "      <th>Mentions</th>\n",
       "      <th>processed_tokens</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[come, border, kill]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[get, borderland, kill]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[come, borderland, murder]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[get, borderland, 2, murder]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[get, borderland, murder]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   URLs  Mentions              processed_tokens  label\n",
       "0     0         0          [come, border, kill]      0\n",
       "1     0         0       [get, borderland, kill]      0\n",
       "2     0         0    [come, borderland, murder]      0\n",
       "3     0         0  [get, borderland, 2, murder]      0\n",
       "4     0         0     [get, borderland, murder]      0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tokenized = tokenize_and_lemmatize(df_cleaned)\n",
    "df_tokenized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5f3990",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def train_model(df_tokenized):\n",
    "    df = df_tokenized.copy()\n",
    "\n",
    "    # 1. Prepare features and target\n",
    "    X = df['processed_tokens'].apply(lambda x: ' '.join(eval(x) if isinstance(x, str) else x))\n",
    "    y = df['label']\n",
    "\n",
    "    # 2. Split into train/val/test (80/10/10 split)\n",
    "    df_train, df_temp = train_test_split(df, test_size=0.3, stratify=y, random_state=42)\n",
    "    df_val, df_test = train_test_split(df_temp, test_size=0.5, stratify=df_temp['label'], random_state=42)\n",
    "\n",
    "    X_train = df_train['processed_tokens'].apply(lambda x: ' '.join(eval(x) if isinstance(x, str) else x))\n",
    "    X_val = df_val['processed_tokens'].apply(lambda x: ' '.join(eval(x) if isinstance(x, str) else x))\n",
    "    X_test = df_test['processed_tokens'].apply(lambda x: ' '.join(eval(x) if isinstance(x, str) else x))\n",
    "\n",
    "    y_train = df_train['label']\n",
    "    y_val = df_val['label']\n",
    "    y_test = df_test['label']\n",
    "\n",
    "    # 3. TF-IDF vectorization\n",
    "    vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "    X_val_tfidf = vectorizer.transform(X_val)\n",
    "    X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "    # 4. Train the model (no numeric features)\n",
    "    model = RandomForestClassifier(random_state=42)\n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "    # 5. Validation report\n",
    "    print(\"Validation Set Evaluation:\")\n",
    "    y_val_pred = model.predict(X_val_tfidf)\n",
    "    print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "    # 6. Test report\n",
    "    print(\"Test Set Evaluation:\")\n",
    "    y_test_pred = model.predict(X_test_tfidf)\n",
    "    print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "    return model, vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0a057740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set Evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.97      0.95      7015\n",
      "           1       0.93      0.83      0.88      3096\n",
      "\n",
      "    accuracy                           0.93     10111\n",
      "   macro avg       0.93      0.90      0.92     10111\n",
      "weighted avg       0.93      0.93      0.93     10111\n",
      "\n",
      "Test Set Evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.97      0.95      7016\n",
      "           1       0.93      0.83      0.88      3096\n",
      "\n",
      "    accuracy                           0.93     10112\n",
      "   macro avg       0.93      0.90      0.91     10112\n",
      "weighted avg       0.93      0.93      0.93     10112\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model, vectorizer = train_model(df_tokenized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1386be3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def train_model(df_tokenized):\n",
    "    df = df_tokenized.copy()\n",
    "\n",
    "    # 1. Prepare features and target\n",
    "    X = df['processed_tokens'].apply(lambda x: ' '.join(eval(x) if isinstance(x, str) else x))\n",
    "    y = df['label']\n",
    "\n",
    "    # 2. Split into train/val/test (80/10/10 split)\n",
    "    df_train, df_temp = train_test_split(df, test_size=0.3, stratify=y, random_state=42)\n",
    "    df_val, df_test = train_test_split(df_temp, test_size=0.5, stratify=df_temp['label'], random_state=42)\n",
    "\n",
    "    X_train = df_train['processed_tokens'].apply(lambda x: ' '.join(eval(x) if isinstance(x, str) else x))\n",
    "    X_val = df_val['processed_tokens'].apply(lambda x: ' '.join(eval(x) if isinstance(x, str) else x))\n",
    "    X_test = df_test['processed_tokens'].apply(lambda x: ' '.join(eval(x) if isinstance(x, str) else x))\n",
    "\n",
    "    y_train = df_train['label']\n",
    "    y_val = df_val['label']\n",
    "    y_test = df_test['label']\n",
    "\n",
    "    # 3. TF-IDF vectorization\n",
    "    vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "    X_val_tfidf = vectorizer.transform(X_val)\n",
    "    X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "    # 4. Train the model (no numeric features)\n",
    "    # model = RandomForestClassifier(random_state=42)\n",
    "    model = LogisticRegression(max_iter=1000)\n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "    # 5. Validation report\n",
    "    print(\"Validation Set Evaluation:\")\n",
    "    y_val_pred = model.predict(X_val_tfidf)\n",
    "    print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "    # 6. Test report\n",
    "    print(\"Test Set Evaluation:\")\n",
    "    y_test_pred = model.predict(X_test_tfidf)\n",
    "    print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "    return model, vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5c68cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set Evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.93      0.88      7015\n",
      "           1       0.79      0.60      0.68      3096\n",
      "\n",
      "    accuracy                           0.83     10111\n",
      "   macro avg       0.81      0.77      0.78     10111\n",
      "weighted avg       0.82      0.83      0.82     10111\n",
      "\n",
      "Test Set Evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.93      0.88      7016\n",
      "           1       0.79      0.60      0.68      3096\n",
      "\n",
      "    accuracy                           0.83     10112\n",
      "   macro avg       0.82      0.77      0.78     10112\n",
      "weighted avg       0.83      0.83      0.82     10112\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model, vectorizer = train_model(df_tokenized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9e87474d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Make sure these are downloaded once\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def preprocess_post(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    lemmatized = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tags]\n",
    "    return ' '.join(lemmatized)\n",
    "\n",
    "def predict_post(text, model, vectorizer):\n",
    "    processed_text = preprocess_post(text)\n",
    "    tfidf_features = vectorizer.transform([processed_text])\n",
    "    prediction = model.predict(tfidf_features)[0]\n",
    "    return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "2db36e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: 1\n"
     ]
    }
   ],
   "source": [
    "text = input()\n",
    "label = predict_post(text, model, vectorizer)\n",
    "print(\"Predicted label:\", label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "452ea848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set Evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.97      0.95      7360\n",
      "           1       0.92      0.78      0.84      2753\n",
      "\n",
      "    accuracy                           0.92     10113\n",
      "   macro avg       0.92      0.88      0.89     10113\n",
      "weighted avg       0.92      0.92      0.92     10113\n",
      "\n",
      "Test Set Evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.97      0.95      7361\n",
      "           1       0.92      0.77      0.84      2752\n",
      "\n",
      "    accuracy                           0.92     10113\n",
      "   macro avg       0.92      0.87      0.89     10113\n",
      "weighted avg       0.92      0.92      0.92     10113\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "target = 'Positive'\n",
    "\n",
    "df = pd.read_csv('twitter_training.csv').iloc[:,2:4]\n",
    "\n",
    "df.columns = ['label', 'post_text']\n",
    "\n",
    "df['label'] = df.pop('label')\n",
    "\n",
    "df['label'] = (df['label'] == target).astype(int)\n",
    "\n",
    "df_cleaned = preprocess_twitter_df(df)\n",
    "\n",
    "df_tokenized = tokenize_and_lemmatize(df_cleaned)\n",
    "\n",
    "model, vectorizer = train_model(df_tokenized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "5c186389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: 1\n"
     ]
    }
   ],
   "source": [
    "text = input()\n",
    "label = predict_post(text, model, vectorizer)\n",
    "print(\"Predicted label:\", label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c46453d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: 1\n",
      "Predicted label: 0\n",
      "Predicted label: 1\n",
      "Predicted label: 0\n",
      "Predicted label: 1\n"
     ]
    }
   ],
   "source": [
    "text = \"I love you\"\n",
    "label = predict_post(text, model, vectorizer)\n",
    "print(\"Predicted label:\", label)\n",
    "text = \"I hate you\"\n",
    "label = predict_post(text, model, vectorizer)\n",
    "print(\"Predicted label:\", label)\n",
    "text = \"you look great\"\n",
    "label = predict_post(text, model, vectorizer)\n",
    "print(\"Predicted label:\", label)\n",
    "text = \"Fucking weird\"\n",
    "label = predict_post(text, model, vectorizer)\n",
    "print(\"Predicted label:\", label)\n",
    "text = \"you look pretty\"\n",
    "label = predict_post(text, model, vectorizer)\n",
    "print(\"Predicted label:\", label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b89a47c",
   "metadata": {},
   "source": [
    "# Model of new dataset validate old dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b9df34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# target = 'Positive'\n",
    "\n",
    "df = pd.read_csv('twitter_training.csv').iloc[:,2:4]\n",
    "\n",
    "# df.columns = ['label', 'post_text']\n",
    "\n",
    "# df['label'] = df.pop('label')\n",
    "\n",
    "# df['label'] = (df['label'] == target).astype(int)\n",
    "\n",
    "df_cleaned = preprocess_twitter_df(df)\n",
    "\n",
    "df_tokenized = tokenize_and_lemmatize(df_cleaned)\n",
    "\n",
    "model, vectorizer = train_model(df_tokenized)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "studysession",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
